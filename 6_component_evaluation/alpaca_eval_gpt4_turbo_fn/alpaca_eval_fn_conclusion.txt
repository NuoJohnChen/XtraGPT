<|im_start|>system
You are a highly efficient assistant, who evaluates and rank large language models (LLMs) based on the quality of their responses to given prompts. This process will create a leaderboard reflecting the most accurate and human-preferred answers.
<|im_end|>
<|im_start|>user
I require a leaderboard for various large language models. I'll provide you with prompts given to these models and their corresponding responses. Your task is to assess these responses, ranking the models in order of preference from a human perspective. Once ranked, please output the results in a structured JSON format for the make_partial_leaderboard function.

## Prompt

{
    "instruction": """{instruction}""",
}

## Model Outputs

Here are the unordered outputs from the models. Each output is associated with a specific model, identified by a unique model identifier.

{
    {
        "model": "m",
        "output": """{output_1}"""
    },
    {
        "model": "M",
        "output": """{output_2}"""
    }
}

## Task

Evaluate and rank the models based on the quality and relevance of their outputs. The ranking should be such that the model with the highest quality output is ranked first.
Criteria 1. Broader Impact and Future Directions Explanation 1. Assess the thoroughness of the paper's conclusion or discussion sections in addressing the broader impact of the research, including societal, technological, regulatory, or academic implications. Does the paper provide specific and clear avenues for future work that build upon the current findings? Are these points presented cohesively and with enough detail to demonstrate the study’s relevance beyond its immediate scope?\nCriteria 2. Clarity and Impact of Key Innovations and Findings Explanation 2. Evaluate whether the conclusion effectively highlights and summarizes the paper’s key innovations and major findings. Does it clearly restate the novel aspects of the work (e.g., methodologies, frameworks, or algorithms) and present concrete results or evidence of improvements over existing approaches? Does the conclusion provide sufficient detail on how these innovations contribute to the field, including any quantitative or qualitative outcomes, and compare them to previous work to demonstrate significance?
<|im_end|>